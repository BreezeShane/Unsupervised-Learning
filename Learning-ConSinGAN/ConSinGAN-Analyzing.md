# 针对ConSinGAN的一次大剖析

## 针对接收参数的处理

首先调用get_argument函数来接受处理命令行传来的参数。其中get_argument函数定义在config.py文件里，不难看出这个文件用以配置一些重要参数。该函数使用了python用于解析命令行参数和选项的标准模块argparse来创建ArgumentParser对象，并在后续的add_argument()方法给该对象添加参数。最后调用parse_args()方法进行解析，成功之后即可使用。

做完这些之后又调用了functions.py内的post_config函数，用来讲参数信息传送给程序用以配置，所以才会出现许多的针对opt各个属性的判断。良好地实现了数据交接的过程。后来又调用了generate_dir2save函数，具体而详细地给每一个训练模型做好了分类，便于用户使用。

此后又使用了文件I/O来生成parameters.txt，估计是用来与其他部分共享配置信息用的。然后开始将主程序的py文件（其实也包含pyc文件）自复制到相应模型下的目录中，并打印出来。为了体现信息的完备，也加入了计时器，用以计算训练所消耗的时间。

不过注意到一个细节——作者在判断模式的时候是将generation、retarget和animation放在一起，而harmonization和editing放在一起，大致可以猜出本质上讲，这五个模式在初始阶段只分两流。另外，当启用animation模式的时候程序则设定最小尺寸（个人认为是在此处设定训练起点）

> 「附言」：可能你觉得这块的内容没什么用，跟网络一点关系都没有，其实不然。一则我们可以通过这里表现的信息大致把握网络的相关参数，再则我们也能学习到优秀的代码规范，代码规范的重要性不必多言。

## 训练模型

### Generation生成图像模式

第一次剖析的时候我启用了这个模式，现在我们来看看这发生了什么。

可以注意到，在training_generation.py里import了models.py内定义的函数。而models.py定义了权重初始化函数、获取激活函数等。（现在还无法确定upsample是做什么用的，更不用提建立ConvBlock类、Discriminator类和GrowingGenerator类是干什么的了。。。）

training_generation.py定义了train函数、train_single_scale函数、generate_samples函数、init_G函数和init_D函数，具体功用后续记录。

网络的训练是从train函数开始的，并且传入的参数是先前建立好的opt对象，并进行了以下各步骤：

1. 首先把相关参数信息都输出出来，便于审查。

2. 调用functions内的read_image函数读取opt.input_name指定路径下的图像，并把该图像的类型（np类型）转换成torch的tensor类型并做了一次翻转处理，为什么这么转我就不知道了，或许论文里会有答案吧。。。

   > 其实中间的切片操作我一开始还真没咋明白做了什么，
   >
   > ```python
   > x = x[:,:,:,None]
   > ```
   >
   > 现在才知道`None`表示该维不进行切片，而是将该维整体作为数组元素处理。

3. 然后又把模型放到GPU上进行并做了类型转换，转换为FloatTensor，继续将数据normalize了一下，并将结果限制在[-1, 1]之间。读取图像的最后一步就是获取该张量第二维度上的切片（**为何这样做我不清楚，可能是希望获取RGB通道上的对应数值吗？这里先留下疑问。**）并返回。

4. 调用functions内的adjust_scales2image函数，似乎起到的作用应该是缩减图像尺寸、分辨率的，因为这里面调用了imresize函数。而在imresize函数这里又调用了torch2uint8函数。**然而这并非像名字那样看上去简单，除了转换类型它还做了不少事：**首先在第一维度上只取了第一个元素，然后将空间矩阵翻转「即由原来的(0, 1, 2)翻转到(1,2,0)」，再将数据转移到CPU上先转换成numpy类型（因为numpy只能在CPU上运行），然后做了一系列的逆操作——先把数据denormalize化并限制在(0, 1)之间，再与255相乘，最后才将数据转换成unsigned int8类型。继续在imresize函数内调用了imresize_in函数，里面又继续调用fix_scale_and_size函数，先是把标量因子复制后做成列表，然后又添加了一个值「这个值代表什么意义我现在还无法得知」到列表的最后一位。并判断得知输出形状为空的时候，程序将输入形状转为array型后向上取整并转成unsigned int类型，最终把标量因子和输出形状传出。接着开始加载方法与函数指针组合而成的字典并获取当前采用的kernel。之后计算了antialiasing的值。「可能第一次运行，这个变量确实不能成立，而在后续某一步的时候才能成立，当然这个量代表着什么我还是不清楚，不过作者已经注释了——仅当缩小规模的时候抗锯齿化才会被使用。不过这已经多少体现了GAN在学习的时候是逐渐将图像压缩下来的。」接着建立起sorted_dims这个列表，运用np.argsort方法将列表内各元素的值做了排序，将图像数组复制过后遍历sorted_dims，「直到这里我才明白为什么作者不去直接给各元素排序，一方面是避免过多消耗内存，另一方面则是保证程序的简洁性。也不难明白这个列表排序的意义不大，否则作者不会采用这种方法。」第一次运行，sorted_dims内各元素都为1，因此不会做后续的事而直接返回输出。「恐怕作者之所以要复制图像张量是因为循环内部要迭代更新，在一个循环周期内需要同时使用原来的属性信息和迭代后的属性信息。」最后回到imresize函数调用了np2torch函数，意在将numpy类型转换为torch.Tensor类型。「这里回用了第二步的函数。」最后又重新更新了scale_factor这个标量因子，「这是在干啥?......」，至此，adjust_scales2image函数才正式执行完毕。

5. 继续开始执行下一个命令调用functions.create_reals_pyramid函数，依旧传入图像和opt对象。这次先新声明了reals空列表，然后由于目前的模式是generative，所以直接跳入下面的循环里，粗略一看，这里便出现了针对图像规模的迭代过程——不断更新规模指标，并不断重用imresize函数来实现图像规模压缩迭代，并将每一个图片张量加入到reals空列表中，返回这个reals图像张量集合。调用了contributions函数，先是定义了lambda arg函数，然后确定核心宽度，接着生成了从1开始到输出长度与1的和为止的numpy数组以及左边界数组。继续设定扩大后的边界宽度，然后将左边界数组扩增维度，并生成了与扩增后核心宽度相关的数组与其相加（发生了传播），在此基础上减去1，最后又删除了多余维度（可能是这个意思？）。

   > 作者在设定扩大后边界宽度的时候注释了如下内容：
   >
   > 核心宽度需要扩大，因为当覆盖面含有亚像素边界的时候，它（程序）必须能“看到”它仅覆盖的部分像素簇的像素中心。因此我们在每一边添加了一个像素来考虑这个情况。（权重可以将这些添加的像素趋零化。
   >
   > 在做后续操作的时候又注释了以下内容：
   >
   > 给每一个输出位置确定了一系列的视界，这些视界都是输入图像的在输出图像中的像素点所看到的像素簇。我们在这得到了一个矩阵，这个矩阵的视界维度是输出的大像素簇，并且正交维度是它所看到的像素（核心尺寸 + 2）
   >
   > 其中有亚像素（Sub-Pixel）的概念，我先后查了好些个相关介绍，总算有一个解释比较完备的了：[亚像素Sub Pixel](https://www.cnblogs.com/Jessica-jie/p/8529564.html)
   >
   > 亚像素是相对于通常说的像素而言的，通常的像素是成像面的基本单位也是最小单位，常常被称为图像的物理分辨率。若成像系统要显示的对象尺寸小于物理分辨率，成像系统是无法正常辨识出来的。某CMOS成像芯片在成像时，对物理世界中连续的图像进行了离散化处理，这时成像面上每一个像素点只代表其附近的颜色。而两个像素之间有几微米的距离，在宏观上可以看作是连在一起的，但在微观上它们之间还有无限更小的东西存在，是两个物理像素之间的“像素”，这些更小的东西就称为“亚像素”。实际上，“亚像素”应该是存在的，只是硬件上没有比像素更细微的传感器把它检测出来而已，于是在软件上可以把它近似地计算出来。
   
   现在到这里定义了权重矩阵并初始化，又记录该矩阵各行元素之和，迭代了一次权重矩阵，并根据所输入的长度做了镜像拼接，后来将视界矩阵和镜像矩阵内的对应元素进行了求余运算。
   
   

**（未完待续......）**

### Retarget图形变换模式

### Animation生成GIF动画模式

### Hamonization嵌入和谐化图像模式

### Editing图像修复模式

## 生成模型

## 有关论文的剖析与理解

> [原论文地址](https://openaccess.thecvf.com/content/WACV2021/papers/Hinz_Improved_Techniques_for_Training_Single-Image_GANs_WACV_2021_paper.pdf)